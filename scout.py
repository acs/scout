#!/usr/bin/python
# -*- coding: utf-8 -*-
#
# This script parses IRC logs and stores the extracted data in
# a database
# 
# Copyright (C) 2012-2013 Bitergia
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
#
# Authors:
#   Alvaro del Castillo San Felix <acs@bitergia.com>
#   Santiago Due√±as <sduenas@bitergia.com>
#

import logging
import sys

from datetime import datetime
from optparse import OptionParser

from vizgrimoire.metrics.query_builder import DSQuery
from vizgrimoire.GrimoireUtils import createJSON

from Scout.database import Database

class Error(Exception):
    """Application error."""


def read_options():
    parser = OptionParser(usage="usage: %prog [options]",
                          version="%prog 0.1")
    parser.add_option("--dir",
                      action="store",
                      dest="data_dir",
                      default="irc",
                      help="Directory with all IRC logs")
    parser.add_option("-f", "--file",
                      action="store",
                      dest="events_file",
                      help="File with the events to be loaded")
    parser.add_option("-b", "--backend",
                      action="store",
                      dest="backend",
                      help="Backend to use for the events (stackoverflow, github, ...)")
    parser.add_option("-j","--json",
                      action="store",
                      dest="json_file",
                      default="events.json",
                      help="Generate a JSON file with the events.")
    parser.add_option("-d", "--database",
                      action="store",
                      dest="dbname",
                      help="Database where information is stored")
    parser.add_option("-u", "--db-user",
                      action="store",
                      dest="dbuser",
                      default="root",
                      help="Database user")
    parser.add_option("-p", "--db-password",
                      action="store",
                      dest="dbpassword",
                      default="",
                      help="Database password")
    parser.add_option("-g", "--debug",
                      action="store_true",
                      dest="debug",
                      default=False,
                      help="Debug mode")


    (opts, args) = parser.parse_args()

    if len(args) != 0:
        parser.error("Wrong number of arguments")

    if not (opts.dbname and opts.dbuser):
        parser.error("--database --db-user are needed")

    return opts


def string_to_datetime(s, schema):
    """Convert string to datetime object"""
    try:
        return datetime.strptime(s, schema)
    except ValueError:
        raise Error("Parsing date %s to %s format" % (s, schema))

def load_csv_file(filepath, db, backend):
    """Load a CSV events file in MySQL."""
    import csv
    if backend not in ("stackoverflow","github","mail"):
        raise ("Backend not supported: " + backend)
    try:
        infile = open(filepath, 'rt')
    except EnvironmentError as e:
        raise Error("Cannot open %s for reading: %s" % (filepath, e))

    count_events = 0
    count_events_new = 0
    # last_date = db.get_last_date(channel_id)

    if backend == "github":
        # Don't use CSV module for this format. Not easy to parse with it.
        pass
    elif backend == "mail":
        data = csv.reader(infile, delimiter=',',quotechar='"', escapechar='\\')
    else:
        data = csv.reader(infile, delimiter=',',quotechar='"')

    try:
        fields = None
        if backend in ["mail","stackoverflow"]:
            for event_data in data:
                if backend != "mail":
                    # In mail CSV we don't have fields in the first row
                    if fields is None:
                        # first row are the fields names
                        fields = event_data
                        continue
                # TODO: Loosing " inside the body. To be fixed. Quick hack.
                event_data = [fevent.replace('"','') for fevent in event_data]
                event_data_str = '"'+ '","'.join(event_data)+'"'
                if backend == "stackoverflow":
                    db.stackoverflow_insert_event(event_data_str, fields)
                elif backend == "github":
                    db.github_insert_event(event_data, fields)
                elif backend == "mail":
                    db.mail_insert_event(event_data_str)
            count_events_new += 1
        elif backend in ["github"]:
            for event_data in infile:
                if fields is None:
                    # first row are the fields names
                    fields = event_data[:-1].split(",")
                    continue
                db.github_insert_event(event_data, fields)

    except Exception as e:
        raise Error("Error parsing %s file: %s" % (filepath, e))
    finally:
        infile.close()
    return count_events, count_events_new

def create_events(filepath, backend):
    dsquery = DSQuery(opts.dbuser, opts.dbpassword, opts.dbname)

    def get_stackoverflow_events():
        table = "stackoverflow_events"
        url_posts = "http://stackoverflow.com/questions/"
        # Common fields for all events: date, summmary, url
        q = "SELECT CONCAT('"+url_posts+"',Post_Link) as url, CreationDate as date, title as summary, "
        q += " ViewCount as views, Score as score, PostTypeId as type "
        q += " FROM " + table
        q += " ORDER BY date DESC "
        return dsquery.ExecuteQuery(q)

    def get_github_events():
        table = "github_events"
        # Common fields for all events: date, summmary, url
        q = "SELECT repo_url as url, created_at as date, repo_name as summary, type "
        q += " FROM " + table
        q += " ORDER BY date DESC "
        return dsquery.ExecuteQuery(q)

    def get_mail_events():
        table = "mail_events"
        # Common fields for all events: date, summmary, url
        q = "SELECT url, sent_at as date, subject as summary "
        q += " FROM " + table
        q += " ORDER BY date DESC "
        return dsquery.ExecuteQuery(q)

    if backend == "stackoverflow":
        res = {"stackoverflow":get_stackoverflow_events()}
        createJSON(res, filepath)

    elif backend == "github":
        res = {"github":get_github_events()}
        createJSON(res, filepath)

    elif backend == "mail":
        res = {"mail":get_mail_events()}
        createJSON(res, filepath)

    elif backend is None:
        # Generate all events
        res = {"stackoverflow":get_stackoverflow_events(),
               "github":get_github_events(),
               "mail":get_mail_events()}
        createJSON(res, filepath)

def create_tables (backend):
    if opts.backend == "stackoverflow":
        db.create_tables_stackoverflow()
    elif opts.backend == "github":
        db.create_tables_github()
    elif opts.backend == "mail":
        db.create_tables_mail()
    else:
        logging.error(opts.backend + " not supported")
        raise

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO,format='%(asctime)s %(message)s')

    opts = read_options()
    db = Database(opts.dbuser, opts.dbpassword, opts.dbname)
    db.open_database()

    try:
        if not opts.events_file and opts.json_file:
            create_events(opts.json_file, opts.backend)
        else:
            create_tables(opts.backend)
            load_csv_file(opts.events_file, db, opts.backend)
    except Error as e:
        print(e)

    db.close_database()
    sys.exit(0)